\graphicspath{{tex/appendix/nb/jupyter/detect/}}

\hypertarget{detector-model-kiukas-ruschhaupt-schmidt-werner}{%
\section[Detector model: Kiukas, Ruschhaupt, Schmidt, Werner]{Detector model: Kiukas, Ruschhaupt, \linebreak[4] Schmidt, Werner}
\label{detector-model-kiukas-ruschhaupt-schmidt-werner}}

\begin{lstlisting}[language=Python]
from sympy import *
#from sympy.physics.matrices import mdft
from sympy.physics.quantum import TensorProduct
from sympy.functions.special.delta_functions import Heaviside
from sympy.physics.quantum.dagger import Dagger

from sympy.stats import ContinuousRV, variance, std

from sympy.plotting import plot, plot3d_parametric_line

import numpy as np
import matplotlib
import matplotlib.pyplot as plt

matplotlib.rcParams['text.usetex'] = True
#matplotlib.rcParams['text.latex.preamble'] = r'''
#    \usepackage{DejaVuSans}
#    \usepackage{xparse}
#    \usepackage{amsmath}
#    \usepackage{physics}
#'''
#matplotlib.rcParams['mathtext.fontset'] = 'dejavusans' 
#matplotlib.rcParams['mathtext.default'] = 'sf'
matplotlib.rcParams['figure.dpi'] = 140
# matplotlib.rcParams['figure.figsize'] = (8,8/sqrt(2))
matplotlib.rcParams['axes.labelsize'] = 16

# https://matplotlib.org/gallery/mplot3d/lines3d.html?highlight=parametric
# This import registers the 3D projection, but is otherwise unused.
from mpl_toolkits.mplot3d import Axes3D  # noqa: F401 unused import
\end{lstlisting}

\begin{lstlisting}[language=Python]
gamma = Symbol('gamma', real=True)
t = Symbol('t', real=True)
tprime = Symbol('t\'', real=True)
omega = Symbol('omega', real=True)
nu = Symbol('nu', real=True)
\end{lstlisting}

\begin{lstlisting}[language=Python]
def D(_gamma):
    return Rational(1, 2) * Matrix([
        [0, 0],
        [0, _gamma]
    ])
\end{lstlisting}

\begin{lstlisting}[language=Python]
H = Matrix ([
[0, 1] ,
[1, 0]
])
\end{lstlisting}

\begin{lstlisting}[language=Python]
init_printing ()
\end{lstlisting}

\begin{lstlisting}[language=Python]
H
\end{lstlisting}

\[\left[\begin{matrix}0 & 1\\1 & 0\end{matrix}\right]\]

\begin{lstlisting}[language=Python]
H.eigenvects()
\end{lstlisting}

\[\left [ \left ( -1, \quad 1, \quad \left [ \left[\begin{matrix}-1\\1\end{matrix}\right]\right ]\right ), \quad \left ( 1, \quad 1, \quad \left [ \left[\begin{matrix}1\\1\end{matrix}\right]\right ]\right )\right ]\]

It's manually seen that \(\langle H \rangle = 0\) and
\(\langle H^2 \rangle = 1\), therefore \(\sigma_{H} = 1\).

\begin{lstlisting}[language=Python]
def K(_gamma):
    return H - I*D(_gamma)
\end{lstlisting}

\begin{lstlisting}[language=Python]
K(2*sqrt(2))
\end{lstlisting}

\[\left[\begin{matrix}0 & 1\\1 & - \sqrt{2} i\end{matrix}\right]\]

\begin{lstlisting}[language=Python]
K(2*sqrt(2)).eigenvects()
\end{lstlisting}

\[\left [ \left ( - \frac{\sqrt{2}}{2} - \frac{\sqrt{2} i}{2}, \quad 1, \quad \left [ \left[\begin{matrix}- \frac{1}{\frac{\sqrt{2}}{2} + \frac{\sqrt{2} i}{2}}\\1\end{matrix}\right]\right ]\right ), \quad \left ( \frac{\sqrt{2}}{2} - \frac{\sqrt{2} i}{2}, \quad 1, \quad \left [ \left[\begin{matrix}- \frac{1}{- \frac{\sqrt{2}}{2} + \frac{\sqrt{2} i}{2}}\\1\end{matrix}\right]\right ]\right )\right ]\]

\begin{lstlisting}[language=Python]
def B(_gamma):
    return lambda t: exp(-I*K(_gamma)*t)
\end{lstlisting}

\begin{lstlisting}[language=Python]
def U():
    return lambda t: exp(-I*H*t)
\end{lstlisting}

\begin{lstlisting}[language=Python]
def non_unitary_psi(_t):
    return B(2*sqrt(2))(_t) * Matrix([1,0])
\end{lstlisting}

\begin{lstlisting}[language=Python]
def unitary_psi(_t):
    return U()(_t) * Matrix([1,0])
\end{lstlisting}

\begin{lstlisting}[language=Python]
non_unitary_psi(t)
\end{lstlisting}

\begin{equation}\label{eq:sympy:non-unitary-evol}
    \left[\begin{matrix}\frac{\sqrt{2} i t e^{- \frac{\sqrt{2} t}{2} - \frac{\sqrt{2} i t}{2}}}{2 \left(\frac{\sqrt{2} t}{2} + \frac{\sqrt{2} i t}{2}\right)} - \frac{\sqrt{2} i t e^{- \frac{\sqrt{2} t}{2} + \frac{\sqrt{2} i t}{2}}}{2 \left(\frac{\sqrt{2} t}{2} - \frac{\sqrt{2} i t}{2}\right)}\\\frac{\sqrt{2} e^{- \frac{\sqrt{2} t}{2} - \frac{\sqrt{2} i t}{2}}}{2} - \frac{\sqrt{2} e^{- \frac{\sqrt{2} t}{2} + \frac{\sqrt{2} i t}{2}}}{2}\end{matrix}\right]
\end{equation}

New period

\begin{lstlisting}[language=Python]
2*pi / (sqrt(2)/2)
\end{lstlisting}

\[2 \sqrt{2} \pi\]

Components are either pure real or pure imaginary:

\begin{lstlisting}[language=Python]
plot(re(non_unitary_psi(t)[0]), (t, 0, 10),
     line_color='r', xlabel=r'$t$', ylabel=r'$\mathrm{Re}\left\langle 0 | \psi \right\rangle $')
\end{lstlisting}

\begin{figure}
\centering
\includegraphics[width=0.6\linewidth]{output_20_0.png}
\caption[]{png}
\end{figure}

\begin{lstlisting}
<sympy.plotting.plot.Plot at 0x7fe25ffc9898>
\end{lstlisting}

\begin{lstlisting}[language=Python]
plot(im(non_unitary_psi(t)[1]), (t, 0, 10),
     line_color='b', xlabel=r'$t$', ylabel=r'$\mathrm{Im}\left\langle 1 | \psi \right\rangle $')
\end{lstlisting}

\begin{figure}
\centering
\includegraphics[width=0.6\linewidth]{output_21_0.png}
\caption[]{png}
\end{figure}

\begin{lstlisting}
<sympy.plotting.plot.Plot at 0x7fe25fec1780>
\end{lstlisting}

\begin{lstlisting}[language=Python]
# verify that our manual simplification is correct
#plot(-sqrt(2)*exp(-t*sqrt(2)/2)*sin(t*sqrt(2)/2), (t, 0, 10) )
\end{lstlisting}

\begin{lstlisting}[language=Python]
def lossy_norm(_t):
    psi = B(2*sqrt(2))(_t) * Matrix([1,0])
    return (abs(psi[0]**2) + abs(psi[1]**2))
\end{lstlisting}

\begin{lstlisting}[language=Python]
lossy_norm(t)
\end{lstlisting}

\[\left|{\left(\frac{\sqrt{2} e^{- \frac{\sqrt{2} t}{2} - \frac{\sqrt{2} i t}{2}}}{2} - \frac{\sqrt{2} e^{- \frac{\sqrt{2} t}{2} + \frac{\sqrt{2} i t}{2}}}{2}\right)^{2}}\right| + \left|{\left(\frac{\sqrt{2} i t e^{- \frac{\sqrt{2} t}{2} - \frac{\sqrt{2} i t}{2}}}{2 \left(\frac{\sqrt{2} t}{2} + \frac{\sqrt{2} i t}{2}\right)} - \frac{\sqrt{2} i t e^{- \frac{\sqrt{2} t}{2} + \frac{\sqrt{2} i t}{2}}}{2 \left(\frac{\sqrt{2} t}{2} - \frac{\sqrt{2} i t}{2}\right)}\right)^{2}}\right|\]

\begin{lstlisting}[language=Python]
non_unitary_psi_n = lambdify(t, non_unitary_psi(t), "numpy")
\end{lstlisting}

\begin{lstlisting}[language=Python]
_lossy_norm_n = lambdify(t, lossy_norm(t), "numpy")
def lossy_norm_n(__t):
    # prevent a warning, even if we know it's real
    return np.real(_lossy_norm_n(__t))
\end{lstlisting}

\begin{lstlisting}[language=Python]
lossy_norm_n
\end{lstlisting}

\begin{lstlisting}
<function __main__.lossy_norm_n(__t)>
\end{lstlisting}

\begin{lstlisting}[language=Python]
def non_unitary_psi_renorm_n(_t):
    return non_unitary_psi_n(_t) / np.sqrt(lossy_norm_n(_t))
\end{lstlisting}

\begin{lstlisting}[language=Python]
T = np.linspace(1e-16, 2*np.pi, 2000)
\end{lstlisting}

\begin{lstlisting}[language=Python]
fig = plt.figure(figsize=(8,8))
#fig = plt.figure()


ax = fig.gca(projection='3d')
ax.view_init(10,-45) # rotate 3d point of view

ax.plot(
    np.real(non_unitary_psi_n(T)[0][0]), np.imag(non_unitary_psi_n(T)[1][0]), T,
    linewidth=1.25
)

##ax.legend()

plt.xlabel(r'$\mathrm{Re}\left\langle 0 | \psi \right\rangle$ (pure real)', labelpad=8)
plt.ylabel(r'$\mathrm{Im}\left\langle 1 | \psi \right\rangle$ (pure imag)', labelpad=10)
ax.set_zlabel(r'$t$')
\end{lstlisting}

\begin{lstlisting}
Text(0.5, 0, '$t$')
\end{lstlisting}

\begin{figure}
\centering
\includegraphics[width=0.6\linewidth]{output_30_1.png}
\caption[]{png}
\end{figure}

\begin{lstlisting}[language=Python]
plot(lossy_norm(t),(t, 0, 2*pi), line_color='g',
     ylabel=r'$\left|\psi\right|^2$', xlabel=r'$t$')
\end{lstlisting}

\begin{figure}
\centering
\includegraphics[width=0.6\linewidth]{output_31_0.png}
\caption[]{png}
\end{figure}

\begin{lstlisting}
<sympy.plotting.plot.Plot at 0x7fe25fe6a6a0>
\end{lstlisting}

\begin{lstlisting}[language=Python]
def prob_0_detect(t):
    return abs(non_unitary_psi(t)[0]**2) / lossy_norm(t)
\end{lstlisting}

\begin{lstlisting}[language=Python]
def prob_1_detect(t):
    return abs(non_unitary_psi(t)[1]**2) / lossy_norm(t)
\end{lstlisting}

\begin{lstlisting}[language=Python]
plot(prob_0_detect(t),(t, 0, 2*pi), line_color='r')
\end{lstlisting}

\begin{figure}
\centering
\includegraphics[width=0.6\linewidth]{output_34_0.png}
\caption[]{png}
\end{figure}

\begin{lstlisting}
<sympy.plotting.plot.Plot at 0x7fe25fddca20>
\end{lstlisting}

\begin{lstlisting}[language=Python]
plot(prob_1_detect(t),(t, -0, 2*pi), line_color='b')
\end{lstlisting}

\begin{figure}
\centering
\includegraphics[width=0.6\linewidth]{output_35_0.png}
\caption[]{png}
\end{figure}

\begin{lstlisting}
<sympy.plotting.plot.Plot at 0x7fe25fd87ac8>
\end{lstlisting}

\begin{lstlisting}[language=Python]
plot(re(non_unitary_psi(t)[0])/sqrt(lossy_norm(t)), (t, 0, 2 * 2*sqrt(2)*pi), line_color='r')
\end{lstlisting}

\begin{figure}
\centering
\includegraphics[width=0.6\linewidth]{output_36_0.png}
\caption[]{png}
\end{figure}

\begin{lstlisting}
<sympy.plotting.plot.Plot at 0x7fe25fd03128>
\end{lstlisting}

\begin{lstlisting}[language=Python]
plot(im(non_unitary_psi(t)[1])/sqrt(lossy_norm(t)), (t, 0, 2 * 2*sqrt(2)*pi), line_color='b')
\end{lstlisting}

\begin{figure}
\centering
\includegraphics[width=0.6\linewidth]{output_37_0.png}
\caption[]{png}
\end{figure}

\begin{lstlisting}
<sympy.plotting.plot.Plot at 0x7fe25f98eeb8>
\end{lstlisting}

\begin{lstlisting}[language=Python]
plot(prob_0_detect(t) + prob_1_detect(t),(t, -0.25, 8*pi))
\end{lstlisting}

\begin{figure}
\centering
\includegraphics[width=0.6\linewidth]{output_38_0.png}
\caption[]{png}
\end{figure}

\begin{lstlisting}
<sympy.plotting.plot.Plot at 0x7fe261980c50>
\end{lstlisting}

\begin{lstlisting}[language=Python]
def prob_0_unitary(t):
    return abs(unitary_psi(t)[0]**2)
\end{lstlisting}

\begin{lstlisting}[language=Python]
def prob_1_unitary(t):
    return abs(unitary_psi(t)[1]**2)
\end{lstlisting}

\begin{lstlisting}[language=Python]
plot(prob_0_unitary(t),(t, -0.25, 8*pi), line_color='r')
\end{lstlisting}

\begin{figure}
\centering
\includegraphics[width=0.6\linewidth]{output_41_0.png}
\caption[]{png}
\end{figure}

\begin{lstlisting}
<sympy.plotting.plot.Plot at 0x7fe261a495c0>
\end{lstlisting}

\begin{lstlisting}[language=Python]
plot(prob_1_unitary(t),(t, -0.25, 8*pi), line_color='b')
\end{lstlisting}

\begin{figure}
\centering
\includegraphics[width=0.6\linewidth]{output_42_0.png}
\caption[]{png}
\end{figure}

\begin{lstlisting}
<sympy.plotting.plot.Plot at 0x7fe25ff64710>
\end{lstlisting}

\begin{lstlisting}[language=Python]
lossy_norm_n(2)
\end{lstlisting}

\[0.19265133139031912\]

\begin{lstlisting}[language=Python]
X = np.linspace(1e-6, 2*np.pi, 1000)  # avoid singularity in t=0
\end{lstlisting}

\begin{lstlisting}[language=Python]
Y = lossy_norm_n(X)
\end{lstlisting}

\begin{lstlisting}[language=Python]
plt.xlabel('$t$')
plt.ylabel(r'$ - \mathrm{d}|\psi|^2 / \mathrm{d}t $')
plt.plot(X, -np.gradient(Y, X), 'g')
\end{lstlisting}

\begin{lstlisting}
[<matplotlib.lines.Line2D at 0x7fe25f7ddba8>]
\end{lstlisting}

\begin{figure}
\centering
\includegraphics[width=0.6\linewidth]{output_46_1.png}
\caption[]{png}
\end{figure}

\begin{lstlisting}[language=Python]
# we have set gamma = 2*sqrt(2)
def hatpsi(_t):
    return \
        Heaviside(_t) * \
        2**(Rational(3,4)) * \
        Matrix([
            [0, 0],
            [0, 1]
        ]) * \
        non_unitary_psi(_t)
        
def hatpsi_n(_t):
    return \
        np.heaviside(_t, 0) * \
        2**(3/4) * \
        np.array([
            [0, 0],
            [0, 1]
        ]) * \
        non_unitary_psi_n(_t)
        
        
    
\end{lstlisting}

\begin{lstlisting}[language=Python]
hatpsi(t)
\end{lstlisting}

\begin{equation}\label{eq:sympy:hatpsi}
    \left[\begin{matrix}0\\2^{\frac{3}{4}} \left(\frac{\sqrt{2} e^{- \frac{\sqrt{2} t}{2} - \frac{\sqrt{2} i t}{2}}}{2} - \frac{\sqrt{2} e^{- \frac{\sqrt{2} t}{2} + \frac{\sqrt{2} i t}{2}}}{2}\right) \theta\left(t\right)\end{matrix}\right]
\end{equation}

\begin{lstlisting}[language=Python]
def hatpsisquarednorm(_t):
    return abs(hatpsi(_t)[0]**2) + abs(hatpsi(_t)[1]**2)

def hatpsisquarednorm_n(_t):
    return abs(hatpsi_n(_t)[0]**2) + abs(hatpsi_n(_t)[1]**2)
\end{lstlisting}

\begin{lstlisting}[language=Python]
hatpsisquarednorm(-1)
\end{lstlisting}

\[0\]

\begin{lstlisting}[language=Python]
plot(hatpsisquarednorm(t), (t, -1, 2*pi), line_color='g',
     ylabel=r'$ \left|\hspace{-.15em}\left|\hat{\psi}\right|\hspace{-.15em}\right|^2 $ =  $ - \mathrm{d}\left|\hspace{-0.15em}\left|\psi\right|\hspace{-0.15em}\right|^2 / \mathrm{d}t $',
     xlabel=r'$t$'
    )
\end{lstlisting}

\begin{figure}
\centering
\includegraphics[width=0.6\linewidth]{output_51_0.png}
\caption[]{png}
\end{figure}

\begin{lstlisting}
<sympy.plotting.plot.Plot at 0x7fe25f7b05f8>
\end{lstlisting}

\begin{lstlisting}[language=Python]
#plot(prob_1_detect(t), hatpsisquarednorm(t), (t, -0.25, 8*pi))
\end{lstlisting}

\begin{lstlisting}[language=Python]
def prob_0_hatpsi(_t):
    return abs(hatpsi(_t)[0]**2) / (abs(hatpsi(_t)[0]**2) + abs(hatpsi(_t)[1]**2))
\end{lstlisting}

\begin{lstlisting}[language=Python]
def prob_1_hatpsi(_t):
    return abs(hatpsi(_t)[1]**2) / (abs(hatpsi(_t)[0]**2) + abs(hatpsi(_t)[1]**2))
\end{lstlisting}

\begin{lstlisting}[language=Python]
plot( abs(hatpsi(t)[1]**2), (t, -2, 2*pi), line_color='b')
\end{lstlisting}

\begin{figure}
\centering
\includegraphics[width=0.6\linewidth]{output_55_0.png}
\caption[]{png}
\end{figure}

\begin{lstlisting}
<sympy.plotting.plot.Plot at 0x7fe25f7797f0>
\end{lstlisting}

\begin{lstlisting}[language=Python]
def fhatpsi1(_nu):
    return fourier_transform(hatpsi(t)[1], t, _nu)
\end{lstlisting}

\begin{lstlisting}[language=Python]
simplify(fhatpsi1(nu))
\end{lstlisting}

\[- \frac{2^{\frac{3}{4}} i}{- 4 \pi^{2} \nu^{2} + 2 \sqrt{2} i \pi \nu + 1}\]

\begin{lstlisting}[language=Python]
plot(abs(fhatpsi1(nu))**2, (nu, -1, 1), line_color='#bbbbbb')
\end{lstlisting}

\begin{figure}
\centering
\includegraphics[width=0.6\linewidth]{output_58_0.png}
\caption[]{png}
\end{figure}

\begin{lstlisting}
<sympy.plotting.plot.Plot at 0x7fe25f63ac18>
\end{lstlisting}

The above Fourier transform is defined in frequency ($\nu$) not angular
frequency ($\omega$), therefore needs rescaling.

\begin{lstlisting}[language=Python]
def fhatpsiomega(_omega):
    return fhatpsi1(_omega/(2*pi)) / sqrt((2*pi))
\end{lstlisting}

\begin{lstlisting}[language=Python]
fhatpsiomega(omega)
\end{lstlisting}

\begin{equation}\label{eq:fhatpsi1_omega}
    - \frac{\sqrt[4]{2} i}{\sqrt{\pi} \left(- \omega^{2} + \sqrt{2} i \omega + 1\right)}
\end{equation}

\begin{lstlisting}[language=Python]
abs(fhatpsiomega(omega))**2
\end{lstlisting}

\[- \frac{\sqrt{2}}{\pi \left(- \omega^{4} - 1\right)}\]

\begin{lstlisting}[language=Python]
integrate(abs(fhatpsiomega(omega))**2, (omega, -oo, +oo))
\end{lstlisting}

\[1\]

\begin{lstlisting}[language=Python]
plot(abs(fhatpsiomega(omega))**2, (omega, -2*pi, 2*pi), line_color='magenta', 
     xlabel=r'$\omega$', ylabel=r'$P(\omega)$')
\end{lstlisting}

\begin{figure}
\centering
\includegraphics[width=0.6\linewidth]{output_64_0.png}
\caption[]{png}
\end{figure}

\begin{lstlisting}
<sympy.plotting.plot.Plot at 0x7fe25dad3860>
\end{lstlisting}

\begin{lstlisting}[language=Python]
# graphical comparison with a normalized gaussian
sigma = 1.0
plot((1/(sqrt(2*pi)*sigma)) * exp(-omega**2/(2*(sigma)**2)), (omega, -2*pi, 2*pi), line_color='magenta')
\end{lstlisting}

\begin{figure}
\centering
\includegraphics[width=0.6\linewidth]{output_65_0.png}
\caption[]{png}
\end{figure}

\begin{lstlisting}
<sympy.plotting.plot.Plot at 0x7fe25dae10f0>
\end{lstlisting}

\hypertarget{discrete-page-wootters-model}{%
\subsection{(Discrete) Page-Wootters
model}\label{discrete-page-wootters-model}}

\begin{lstlisting}[language=Python]
from scipy.linalg import dft, norm, expm
from scipy import stats
\end{lstlisting}

\begin{lstlisting}[language=Python]
T = np.diag(np.arange(0,32)) * np.pi / 16
\end{lstlisting}

\begin{lstlisting}[language=Python]
# The NumPy Fourier matrix is the conjugate of Mathematica's one,
# hence the trailing .conj() 
F = dft(32, scale='sqrtn').conj()
\end{lstlisting}

\begin{lstlisting}[language=Python]
F_dagger = F.conj().T
\end{lstlisting}

\begin{lstlisting}[language=Python]
Omega = F @ T @ F_dagger * 16 / np.pi
\end{lstlisting}

\begin{lstlisting}[language=Python]
oeigenvalues, oeigenvectors = np.linalg.eig(Omega)
\end{lstlisting}

\begin{lstlisting}[language=Python]
np.round(oeigenvalues)
\end{lstlisting}

\begin{lstlisting}
array([-0.+0.j, 31.+0.j,  1.+0.j, 30.+0.j,  2.+0.j, 29.-0.j,  3.+0.j,
       28.-0.j,  4.-0.j, 27.-0.j,  5.-0.j, 26.+0.j,  6.-0.j, 25.+0.j,
        7.-0.j,  8.+0.j, 24.+0.j,  9.-0.j, 23.+0.j, 10.+0.j, 22.+0.j,
       11.+0.j, 21.-0.j, 12.+0.j, 13.-0.j, 20.-0.j, 14.-0.j, 15.-0.j,
       19.+0.j, 16.-0.j, 17.+0.j, 18.-0.j])
\end{lstlisting}

\begin{lstlisting}[language=Python]
H = np.array([
    [0, 1],
    [1, 0]
])
\end{lstlisting}

\begin{lstlisting}[language=Python]
D = np.array([
    [0, 0],
    [0, np.sqrt(2)]
])
\end{lstlisting}

\begin{lstlisting}[language=Python]
K = H - 1j*D
\end{lstlisting}

\begin{lstlisting}[language=Python]
K
\end{lstlisting}

\begin{lstlisting}
array([[0.+0.j        , 1.+0.j        ],
       [1.+0.j        , 0.-1.41421356j]])
\end{lstlisting}

\begin{lstlisting}[language=Python]
J = np.kron(Omega, np.eye(2)) + np.kron(np.eye(32), K)
\end{lstlisting}

\begin{lstlisting}[language=Python]
eigenvalues, eigenvectors = np.linalg.eig(J)
\end{lstlisting}

\begin{lstlisting}[language=Python]
EnergyCorrectionMatrices = np.zeros((64, 64, 64), np.complex)
for n in range(64):
    #EnergyCorrectionMatrices[n] = np.kron(
    #    expm(-1j*eigenvalues[n]*T),
    #    np.eye(2)
    #)
    EnergyCorrectionMatrices[n] = expm(-1j*eigenvalues[n]*np.kron(T, np.eye(2)))
# TODO: DRY
EnergyCorrectionMatricesT = np.zeros((64, 32, 32), np.complex)
for n in range(64):
    EnergyCorrectionMatricesT[n] = expm(-1j*eigenvalues[n]*T)
\end{lstlisting}

\begin{lstlisting}[language=Python]
def history_vector(eigenindex):
    # Needs matrix transposition ".T" (different convention as opposed to Mathematica)
    eigenvector = eigenvectors.T[eigenindex]
    return EnergyCorrectionMatrices[eigenindex] @ eigenvector

# "unflatten" the history_vector v into a a sequence of qubit component pairs
def reshape(v):
    return np.reshape(v, (-1,2))

# also make the first component real
def normalize_initial(v):
    vout = np.zeros(64, np.complex)
    # A phase factor to make it real
    vout = v * np.exp(-1j * np.angle(v[0]))
    # And a factor to normalize the initial state
    vout = vout / sqrt(
        np.abs(vout[0]**2) + np.abs(vout[1]**2)
    )
    return vout
\end{lstlisting}

\begin{lstlisting}[language=Python]
# Find the best linear combination to obtain |0> as initial state
def find_best():
    max_prob0 = 0
    max_prob0_i = 0
    max_prob0_j = 0
    for i in range(32):
        for j in range(32):
            qbi = reshape(history_vector(i))
            qbj = reshape(history_vector(j))
            qbit_hist = qbi + qbj
            prob0 = np.abs(qbit_hist[0][0]**2) / (
                np.abs(qbit_hist[0][0]**2) + np.abs(qbit_hist[0][1]**2)
            )
            if prob0 > max_prob0:
                max_prob0 = prob0
                max_prob0_i = i
                max_prob0_j = j
    print (max_prob0_i, max_prob0_j, max_prob0)
    return (max_prob0_i, max_prob0_j)
    
\end{lstlisting}

\begin{lstlisting}[language=Python]
# start with |0> as close as possible
i, j = find_best()
qbhistvec = normalize_initial(history_vector(i) + history_vector(j))
qbhist = reshape(qbhistvec) 
\end{lstlisting}

\begin{lstlisting}
1 21 1.0
\end{lstlisting}

\begin{lstlisting}[language=Python]
qbhist = qbhist.astype(complex)
\end{lstlisting}

Consitently with ``odinary QM'' findings, the component along
\textbar0\textgreater{} stays purely real, and the component along
\textbar1\textgreater{} stays purely imaginary.

\begin{lstlisting}[language=Python]
# Fill data for plotting
times = np.arange(0, 2*np.pi, np.pi/16)
norms = np.zeros(32)
probs0 = np.zeros(32)
probs1 = np.zeros(32)
# Components 0 are pure real, componets 1 are pure imag
real_parts0 = np.real(qbhist.T[0])
imag_parts1 = np.imag(qbhist.T[1])

for i in range(0, 32):
    norms[i] = (np.abs(qbhist[i][0]**2) + np.abs(qbhist[i][1]**2))
    probs0[i] = np.abs(qbhist[i][0]**2) / (
        np.abs(qbhist[i][0]**2) + np.abs(qbhist[i][1]**2) )
    probs1[i] = np.abs(qbhist[i][1]**2) / (
        np.abs(qbhist[i][0]**2) + np.abs(qbhist[i][1]**2) )
\end{lstlisting}

\begin{lstlisting}[language=Python]
plt.ylabel(r'$\mathrm{Re}{\;}_{T}\hspace{-.2em}\left\langle t | {}_{S}\hspace{-.2em}\left\langle 0 | \Psi \right\rangle\hspace{-.17em}\right\rangle $')
plt.xlabel(r'$t$')
plt.plot(times, real_parts0/norms[0], 'rs')
\end{lstlisting}

\begin{lstlisting}
[<matplotlib.lines.Line2D at 0x7fe25d36e438>]
\end{lstlisting}

\begin{figure}
\centering
\includegraphics[width=0.6\linewidth]{output_87_1.png}
\caption[]{png}
\end{figure}

\begin{lstlisting}[language=Python]
plt.ylabel(r'$\mathrm{Im}{\;}_{T}\hspace{-.2em}\left\langle t | {}_{S}\hspace{-.2em}\left\langle 1 | \Psi \right\rangle\hspace{-.17em}\right\rangle $')
plt.xlabel(r'$t$')
plt.plot(times, imag_parts1/norms[0], 'bs')
\end{lstlisting}

\begin{lstlisting}
[<matplotlib.lines.Line2D at 0x7fe25d27da90>]
\end{lstlisting}

\begin{figure}
\centering
\includegraphics[width=0.6\linewidth]{output_88_1.png}
\caption[]{png}
\end{figure}

\begin{lstlisting}[language=Python]
plt.plot(times, norms/norms[0], 'g^')
\end{lstlisting}

\begin{lstlisting}
[<matplotlib.lines.Line2D at 0x7ff538fd94e0>]
\end{lstlisting}

\begin{figure}
\centering
\includegraphics[width=0.6\linewidth]{output_89_1.png}
\caption[]{png}
\end{figure}

\begin{lstlisting}[language=Python]
plt.plot(times, probs0, 'rs')
\end{lstlisting}

\begin{lstlisting}
[<matplotlib.lines.Line2D at 0x7ff538f341d0>]
\end{lstlisting}

\begin{figure}
\centering
\includegraphics[width=0.6\linewidth]{output_90_1.png}
\caption[]{png}
\end{figure}

\begin{lstlisting}[language=Python]
plt.plot(times, probs1, 'bs')
\end{lstlisting}

\begin{lstlisting}
[<matplotlib.lines.Line2D at 0x7ff538f05f60>]
\end{lstlisting}

\begin{figure}
\centering
\includegraphics[width=0.6\linewidth]{output_91_1.png}
\caption[]{png}
\end{figure}

\begin{lstlisting}[language=Python]
fig = plt.figure(figsize=(7,7))

#ax = fig.gca(projection='3d')
ax = fig.add_subplot(111, projection='3d')
ax.view_init(10,-45) # rotate 3d point of view

ax.scatter(
    real_parts0, imag_parts1, times
)

##ax.legend()

plt.xlabel(r'$\mathrm{Re}\left\langle 0 | \psi \right\rangle$ (pure real)')
plt.ylabel(r'$\mathrm{Im}\left\langle 1 | \psi \right\rangle$ (pure imag)')
ax.set_zlabel('t')
\end{lstlisting}

\begin{lstlisting}
Text(0.5, 0, 't')
\end{lstlisting}

\begin{figure}
\centering
\includegraphics[width=0.6\linewidth]{output_92_1.png}
\caption[]{png}
\end{figure}

\hypertarget{detection-event}{%
\subsection{Detection event}\label{detection-event}}

\begin{lstlisting}[language=Python]
sqr2D = np.array([
    [0, 0],
    [0, 2**(3/4)]
])
\end{lstlisting}

\begin{lstlisting}[language=Python]
qbhistvec = qbhistvec.astype(np.complex)
\end{lstlisting}

\begin{lstlisting}[language=Python]
sqr2D = sqr2D.astype(np.complex)
\end{lstlisting}

\begin{lstlisting}[language=Python]
i, j = find_best()
\end{lstlisting}
\begin{lstlisting}
    1 21 1.0
\end{lstlisting}

\begin{lstlisting}[language=Python]
prob_detect_v = \
    (np.kron(EnergyCorrectionMatricesT[i], sqr2D) @ eigenvectors.T[i]) + \
    (np.kron(EnergyCorrectionMatricesT[j], sqr2D) @ eigenvectors.T[j])

# normalize
prob_detect_v = prob_detect_v / norm(prob_detect_v)
\end{lstlisting}

\begin{lstlisting}[language=Python]
prob_detect_v
\end{lstlisting}
\begin{lstlisting}[basicstyle=\tiny\ttfamily]
    array([ 0.00000000e+00+0.00000000e+00j,  4.05729696e-14-3.29580328e-14j,
    0.00000000e+00+0.00000000e+00j,  3.36832200e-14+1.26984798e-01j,
    0.00000000e+00+0.00000000e+00j,  3.06907025e-14+2.18919713e-01j,
    0.00000000e+00+0.00000000e+00j,  2.41489201e-14+2.81218057e-01j,
    0.00000000e+00+0.00000000e+00j,  1.80594950e-14+3.18975095e-01j,
    0.00000000e+00+0.00000000e+00j,  1.20048666e-14+3.36874181e-01j,
    0.00000000e+00+0.00000000e+00j,  9.72568181e-15+3.39129482e-01j,
    0.00000000e+00+0.00000000e+00j,  4.71495487e-15+3.29458326e-01j,
    0.00000000e+00+0.00000000e+00j,  7.30731013e-16+3.11076934e-01j,
    0.00000000e+00+0.00000000e+00j, -2.33137990e-15+2.86713992e-01j,
    0.00000000e+00+0.00000000e+00j, -5.20645847e-15+2.58637303e-01j,
    0.00000000e+00+0.00000000e+00j, -4.44962992e-15+2.28689430e-01j,
    0.00000000e+00+0.00000000e+00j, -6.18946566e-15+1.98328975e-01j,
    0.00000000e+00+0.00000000e+00j, -6.10682346e-15+1.68674732e-01j,
    0.00000000e+00+0.00000000e+00j, -4.03206934e-15+1.40550532e-01j,
    0.00000000e+00+0.00000000e+00j, -2.53146101e-15+1.14529119e-01j,
    0.00000000e+00+0.00000000e+00j, -2.58800567e-15+9.09738099e-02j,
    0.00000000e+00+0.00000000e+00j, -1.97906316e-15+7.00770781e-02j,
    0.00000000e+00+0.00000000e+00j, -1.23528338e-15+5.18955215e-02j,
    0.00000000e+00+0.00000000e+00j, -6.26340868e-16+3.63809059e-02j,
    0.00000000e+00+0.00000000e+00j,  2.17479468e-17+2.34072020e-02j,
    0.00000000e+00+0.00000000e+00j, -1.78333164e-16+1.27936759e-02j,
    0.00000000e+00+0.00000000e+00j,  6.08942511e-17+4.32421818e-03j,
    0.00000000e+00+0.00000000e+00j,  2.30528236e-16-2.23682783e-03j,
    0.00000000e+00+0.00000000e+00j,  6.95934298e-16-7.13201872e-03j,
    0.00000000e+00+0.00000000e+00j,  1.04607624e-15-1.06009991e-02j,
    0.00000000e+00+0.00000000e+00j,  1.52453107e-15-1.28731662e-02j,
    0.00000000e+00+0.00000000e+00j,  1.32662476e-15-1.41624569e-02j,
    0.00000000e+00+0.00000000e+00j,  1.11675707e-15-1.46639178e-02j,
    0.00000000e+00+0.00000000e+00j,  1.08196035e-15-1.45517399e-02j,
    0.00000000e+00+0.00000000e+00j,  7.51391562e-16-1.39784710e-02j,
    0.00000000e+00+0.00000000e+00j,  5.59465932e-16-1.30751439e-02j])
\end{lstlisting}

\begin{lstlisting}[language=Python]
imag_prob_ampl_detect = np.imag( prob_detect_v.reshape(-1, 2).transpose()[1] )
\end{lstlisting}

We compare the detection probability amplitude (component along $\ket{1}$, imaginary part)
with the same component of the evolution.
In other words, we compare a probability amplitude over time with
a probability amplitude over space (i.e. being $\ket{1}$ rather than $\ket{0}$).

\begin{lstlisting}[language=Python]
imag_parts1 / imag_prob_ampl_detect
\end{lstlisting}
\begin{lstlisting}[basicstyle=\tiny\ttfamily]
array([-1.34148089, -1.34148089, -1.34148089, -1.34148089, -1.34148089,
    -1.34148089, -1.34148089, -1.34148089, -1.34148089, -1.34148089,
    -1.34148089, -1.34148089, -1.34148089, -1.34148089, -1.34148089,
    -1.34148089, -1.34148089, -1.34148089, -1.34148089, -1.34148089,
    -1.34148089, -1.34148089, -1.34148089, -1.34148089, -1.34148089,
    -1.34148089, -1.34148089, -1.34148089, -1.34148089, -1.34148089,
    -1.34148089, -1.34148089])
\end{lstlisting}
which shows that the two vectors are essentially the same,
up to a renormalization and change of sign.
Of course, probability over time and over space require a different normalization.
The two would be conceptually uncomparable, but an explaination is in \cite[eq. 6]{Maccone:QMOT}.

\begin{lstlisting}[language=Python]
prob_detect = np.zeros(32)
for t_idx in range(32):
    prob_detect[t_idx] = \
        np.abs(prob_detect_v[2*t_idx])**2 + np.abs(prob_detect_v[2*t_idx+1])**2
\end{lstlisting}

\begin{lstlisting}[language=Python]
plt.plot(times, prob_detect * 16 / np.pi, 'bs')
\end{lstlisting}

\begin{lstlisting}
[<matplotlib.lines.Line2D at 0x7ff538e5b550>]
\end{lstlisting}

\begin{figure}
\centering
\includegraphics[width=0.6\linewidth]{output_99_1.png}
\caption[]{png}
\end{figure}

\begin{lstlisting}[language=Python]
detect_fft = \
    np.kron(F, np.eye(2)) @ prob_detect_v
detect_fft = detect_fft / norm(detect_fft)
\end{lstlisting}

\begin{lstlisting}[language=Python]
prob_detect_fft = np.zeros(32)
for o in range(32):
    prob_detect_fft[o] = \
        np.abs(detect_fft[2*o]**2) + \
        np.abs(detect_fft[2*o + 1]**2) 
\end{lstlisting}

\begin{lstlisting}[language=Python]
# Arrays are "rolled" because the second half 
# of the spectrum is identified with
# negative frequencies.
plt.plot(range(-16, 16), np.roll(prob_detect_fft, -16), 'y^')
\end{lstlisting}

\begin{lstlisting}
[<matplotlib.lines.Line2D at 0x7ff538db5ac8>]
\end{lstlisting}

\begin{figure}
\centering
\includegraphics[width=0.6\linewidth]{output_102_1.png}
\caption[]{png}
\end{figure}

% \hypertarget{entropic-uncertainties}{%
% \subsubsection{Entropic uncertainty relation}\label{jupy:entropic-uncertainties}}

% \begin{lstlisting}[language=Python]
% S_t = stats.entropy(prob_detect)
% \end{lstlisting}

% \begin{lstlisting}[language=Python]
% S_omega = stats.entropy(prob_detect_fft)
% \end{lstlisting}

% \begin{lstlisting}[language=Python]
% S_t
% \end{lstlisting}

% \[2.6193337590390438\]

% \begin{lstlisting}[language=Python]
% S_omega
% \end{lstlisting}

% \[1.3471684169765332\]

% \begin{lstlisting}[language=Python]
% S_t + S_omega
% \end{lstlisting}

% \[3.966502176015577\]

% \begin{lstlisting}[language=Python]
% np.log(32)
% \end{lstlisting}

% \[3.4657359027997265\]

% \begin{lstlisting}[language=Python]
% (S_t + S_omega - np.log(32)) / np.log(32)
% \end{lstlisting}

% \[0.14449060380259104\]

% $14\%$ more than the minumum per entropic uncertainty relation.

\hypertarget{use-scipy-routines-to-compute-sigmas}{%
\subsubsection{Use Scipy routines to compute
sigmas}\label{use-scipy-routines-to-compute-sigmas}}

\begin{lstlisting}[language=Python]

xk = range(-16,16)
pk = np.roll(prob_detect_fft, -16)
detect_fft_pdist = stats.rv_discrete(name='prob_detect_fft_minus16', values=(xk, pk))
\end{lstlisting}

\begin{lstlisting}[language=Python]
sigma_omega = detect_fft_pdist.std()
\end{lstlisting}

\begin{lstlisting}[language=Python]
xk = times
pk = prob_detect
detect_pdist = stats.rv_discrete(name='prob_detect', values=(xk, pk))
\end{lstlisting}

\begin{lstlisting}[language=Python]
sigma_t = detect_pdist.std()
\end{lstlisting}

\begin{lstlisting}[language=Python]
sigma_t * sigma_omega
\end{lstlisting}

\[0.7159703170687718\]

It's still quite a bit more than 0.5, i.e.~the minimum
uncertainty\ldots{}

But this is in fact consistent with the paper.
