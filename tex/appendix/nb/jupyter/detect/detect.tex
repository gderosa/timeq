\graphicspath{{tex/appendix/nb/jupyter/detect/}}

\hypertarget{detector-model-kiukas-ruschhaupt-schmidt-werner}{%
\section{Detector model: Kiukas / Ruschhaupt / Schmidt /
Werner}\label{detector-model-kiukas-ruschhaupt-schmidt-werner}}

\begin{lstlisting}[language=Python]
from sympy import *
#from sympy.physics.matrices import mdft
from sympy.physics.quantum import TensorProduct
from sympy.functions.special.delta_functions import Heaviside
from sympy.physics.quantum.dagger import Dagger

from sympy.stats import ContinuousRV, variance, std

from sympy.plotting import plot, plot3d_parametric_line

import numpy as np
import matplotlib
import matplotlib.pyplot as plt

matplotlib.rcParams['text.usetex'] = False

# https://matplotlib.org/gallery/mplot3d/lines3d.html?highlight=parametric
# This import registers the 3D projection, but is otherwise unused.
from mpl_toolkits.mplot3d import Axes3D  # noqa: F401 unused import
\end{lstlisting}

\begin{lstlisting}[language=Python]
gamma = Symbol('gamma', real=True)
t = Symbol('t', real=True)
tprime = Symbol('t\'', real=True)
omega = Symbol('omega', real=True)
nu = Symbol('nu', real=True)
\end{lstlisting}

\begin{lstlisting}[language=Python]
def D(_gamma):
    return Rational(1, 2) * Matrix([
        [0, 0],
        [0, _gamma]
    ])
\end{lstlisting}

\begin{lstlisting}[language=Python]
H = Matrix ([
[0, 1] ,
[1, 0]
])
\end{lstlisting}

\begin{lstlisting}[language=Python]
init_printing ()
\end{lstlisting}

\begin{lstlisting}[language=Python]
H
\end{lstlisting}

\[\left[\begin{matrix}0 & 1\\1 & 0\end{matrix}\right]\]

\begin{lstlisting}[language=Python]
H.eigenvects()
\end{lstlisting}

\[\left [ \left ( -1, \quad 1, \quad \left [ \left[\begin{matrix}-1\\1\end{matrix}\right]\right ]\right ), \quad \left ( 1, \quad 1, \quad \left [ \left[\begin{matrix}1\\1\end{matrix}\right]\right ]\right )\right ]\]

It's manually seen that \(\langle H \rangle = 0\) and
\(\langle H^2 \rangle = 1\), therefore \(\sigma_{H} = 1\).

\begin{lstlisting}[language=Python]
def K(_gamma):
    return H - I*D(_gamma)
\end{lstlisting}

\begin{lstlisting}[language=Python]
K(2*sqrt(2))
\end{lstlisting}

\[\left[\begin{matrix}0 & 1\\1 & - \sqrt{2} i\end{matrix}\right]\]

\begin{lstlisting}[language=Python]
K(2*sqrt(2)).eigenvects()
\end{lstlisting}

\[\left [ \left ( - \frac{\sqrt{2}}{2} - \frac{\sqrt{2} i}{2}, \quad 1, \quad \left [ \left[\begin{matrix}- \frac{1}{\frac{\sqrt{2}}{2} + \frac{\sqrt{2} i}{2}}\\1\end{matrix}\right]\right ]\right ), \quad \left ( \frac{\sqrt{2}}{2} - \frac{\sqrt{2} i}{2}, \quad 1, \quad \left [ \left[\begin{matrix}- \frac{1}{- \frac{\sqrt{2}}{2} + \frac{\sqrt{2} i}{2}}\\1\end{matrix}\right]\right ]\right )\right ]\]

\begin{lstlisting}[language=Python]
def B(_gamma):
    return lambda t: exp(-I*K(_gamma)*t)
\end{lstlisting}

\begin{lstlisting}[language=Python]
def U():
    return lambda t: exp(-I*H*t)
\end{lstlisting}

\begin{lstlisting}[language=Python]
def non_unitary_psi(_t):
    return B(2*sqrt(2))(_t) * Matrix([1,0])
\end{lstlisting}

\begin{lstlisting}[language=Python]
def unitary_psi(_t):
    return U()(_t) * Matrix([1,0])
\end{lstlisting}

\begin{lstlisting}[language=Python]
non_unitary_psi(t)
\end{lstlisting}

\begin{equation}\label{eq:sympy:non-unitary-evol}
    \left[\begin{matrix}\frac{\sqrt{2} i t e^{- \frac{\sqrt{2} t}{2} - \frac{i t}{2} \sqrt{2}}}{2 \left(\frac{\sqrt{2} t}{2} + \frac{i t}{2} \sqrt{2}\right)} - \frac{\sqrt{2} i t e^{- \frac{\sqrt{2} t}{2} + \frac{i t}{2} \sqrt{2}}}{2 \left(\frac{\sqrt{2} t}{2} - \frac{i t}{2} \sqrt{2}\right)}\\\frac{\sqrt{2}}{2} e^{- \frac{\sqrt{2} t}{2} - \frac{i t}{2} \sqrt{2}} - \frac{\sqrt{2}}{2} e^{- \frac{\sqrt{2} t}{2} + \frac{i t}{2} \sqrt{2}}\end{matrix}\right]
\end{equation}    


New period

\begin{lstlisting}[language=Python]
2*pi / (sqrt(2)/2)
\end{lstlisting}

\[2 \sqrt{2} \pi\]

Components are either pure real or pure imaginary:

\begin{lstlisting}[language=Python]
plot(re(non_unitary_psi(t)[0]), (t, 0, 10), line_color='r')
\end{lstlisting}

\begin{figure}
\centering
\includegraphics[width=0.66\linewidth]{output_20_0.png}
\caption[]{png}
\end{figure}

\begin{lstlisting}
<sympy.plotting.plot.Plot at 0x7fbbab0c5898>
\end{lstlisting}

\begin{lstlisting}[language=Python]
plot(im(non_unitary_psi(t)[1]), (t, 0, 10), line_color='b')
\end{lstlisting}

\begin{figure}
\centering
\includegraphics[width=0.66\linewidth]{output_21_0.png}
\caption[]{png}
\end{figure}

\begin{lstlisting}
<sympy.plotting.plot.Plot at 0x7fbbaac550f0>
\end{lstlisting}

\begin{lstlisting}[language=Python]
def lossy_norm(_t):
    psi = B(2*sqrt(2))(_t) * Matrix([1,0])
    return (abs(psi[0]**2) + abs(psi[1]**2))
\end{lstlisting}

\begin{lstlisting}[language=Python]
non_unitary_psi_n = lambdify(t, non_unitary_psi(t), "numpy")
\end{lstlisting}

\begin{lstlisting}[language=Python]
_lossy_norm_n = lambdify(t, lossy_norm(t), "numpy")
def lossy_norm_n(__t):
    # prevent a warning, even if we know it's real
    return np.real(_lossy_norm_n(__t))
\end{lstlisting}

\begin{lstlisting}[language=Python]
lossy_norm_n
\end{lstlisting}

\begin{lstlisting}
<function __main__.lossy_norm_n>
\end{lstlisting}

\begin{lstlisting}[language=Python]
def non_unitary_psi_renorm_n(_t):
    return non_unitary_psi_n(_t) / np.sqrt(lossy_norm_n(_t))
\end{lstlisting}

\begin{lstlisting}[language=Python]
T = np.linspace(1e-16, 2*np.pi, 2000)
\end{lstlisting}

\begin{lstlisting}[language=Python]
fig = plt.figure(figsize=(12,12))

ax = fig.gca(projection='3d')
ax.view_init(10,-45) # rotate 3d point of view

ax.plot(
    np.real(non_unitary_psi_n(T)[0][0]), np.imag(non_unitary_psi_n(T)[1][0]), T,
    linewidth=1.25
)

##ax.legend()

plt.xlabel('Re <0|\u03C8> (pure real)')
plt.ylabel('Im <1|\u03C8> (pure imag)')
ax.set_zlabel('t')
\end{lstlisting}

\begin{lstlisting}
Text(0.5, 0, 't')
\end{lstlisting}

\begin{figure}
\centering
\includegraphics[width=0.66\linewidth]{output_29_1.png}
\caption[]{png}
\end{figure}

\begin{lstlisting}[language=Python]
plot(lossy_norm(t),(t, 0, 2*pi), line_color='g')
\end{lstlisting}

\begin{figure}
\centering
\includegraphics[width=0.66\linewidth]{output_30_0.png}
\caption[]{png}
\end{figure}

\begin{lstlisting}
<sympy.plotting.plot.Plot at 0x7fbbaac4fbe0>
\end{lstlisting}

\begin{lstlisting}[language=Python]
def prob_0_detect(t):
    return abs(non_unitary_psi(t)[0]**2) / lossy_norm(t)
\end{lstlisting}

\begin{lstlisting}[language=Python]
def prob_1_detect(t):
    return abs(non_unitary_psi(t)[1]**2) / lossy_norm(t)
\end{lstlisting}

\begin{lstlisting}[language=Python]
plot(prob_0_detect(t),(t, 0, 2*pi), line_color='r')
\end{lstlisting}

\begin{figure}
\centering
\includegraphics[width=0.66\linewidth]{output_33_0.png}
\caption[]{png}
\end{figure}

\begin{lstlisting}
<sympy.plotting.plot.Plot at 0x7fbbaab14160>
\end{lstlisting}

\begin{lstlisting}[language=Python]
plot(prob_1_detect(t),(t, -0, 2*pi), line_color='b')
\end{lstlisting}

\begin{figure}
\centering
\includegraphics[width=0.66\linewidth]{output_34_0.png}
\caption[]{png}
\end{figure}

\begin{lstlisting}
<sympy.plotting.plot.Plot at 0x7fbbaab8fd30>
\end{lstlisting}

\begin{lstlisting}[language=Python]
plot(re(non_unitary_psi(t)[0])/sqrt(lossy_norm(t)), (t, 0, 2 * 2*sqrt(2)*pi), line_color='r')
\end{lstlisting}

\begin{figure}
\centering
\includegraphics[width=0.66\linewidth]{output_35_0.png}
\caption[]{png}
\end{figure}

\begin{lstlisting}
<sympy.plotting.plot.Plot at 0x7fbbb2a02d30>
\end{lstlisting}

\begin{lstlisting}[language=Python]
plot(im(non_unitary_psi(t)[1])/sqrt(lossy_norm(t)), (t, 0, 2 * 2*sqrt(2)*pi), line_color='b')
\end{lstlisting}

\begin{figure}
\centering
\includegraphics[width=0.66\linewidth]{output_36_0.png}
\caption[]{png}
\end{figure}

\begin{lstlisting}
<sympy.plotting.plot.Plot at 0x7fbbb0f04f60>
\end{lstlisting}

\begin{lstlisting}[language=Python]
plot(prob_0_detect(t) + prob_1_detect(t),(t, -0.25, 8*pi))
\end{lstlisting}

\begin{figure}
\centering
\includegraphics[width=0.66\linewidth]{output_37_0.png}
\caption[]{png}
\end{figure}

\begin{lstlisting}
<sympy.plotting.plot.Plot at 0x7fbbb2ac52e8>
\end{lstlisting}

\begin{lstlisting}[language=Python]
def prob_0_unitary(t):
    return abs(unitary_psi(t)[0]**2)
\end{lstlisting}

\begin{lstlisting}[language=Python]
def prob_1_unitary(t):
    return abs(unitary_psi(t)[1]**2)
\end{lstlisting}

\begin{lstlisting}[language=Python]
plot(prob_0_unitary(t),(t, -0.25, 8*pi), line_color='r')
\end{lstlisting}

\begin{figure}
\centering
\includegraphics[width=0.66\linewidth]{output_40_0.png}
\caption[]{png}
\end{figure}

\begin{lstlisting}
<sympy.plotting.plot.Plot at 0x7fbbb4f3ad68>
\end{lstlisting}

\begin{lstlisting}[language=Python]
plot(prob_1_unitary(t),(t, -0.25, 8*pi), line_color='b')
\end{lstlisting}

\begin{figure}
\centering
\includegraphics[width=0.66\linewidth]{output_41_0.png}
\caption[]{png}
\end{figure}

\begin{lstlisting}
<sympy.plotting.plot.Plot at 0x7fbbb2cb4400>
\end{lstlisting}

\begin{lstlisting}[language=Python]
lossy_norm_n(2)
\end{lstlisting}

\[0.19265133139031912\]

\begin{lstlisting}[language=Python]
X = np.linspace(1e-6, 2*np.pi, 1000)  # avoid singularity in t=0
\end{lstlisting}

\begin{lstlisting}[language=Python]
Y = lossy_norm_n(X)
\end{lstlisting}

\begin{lstlisting}[language=Python]
plt.plot(X, -np.gradient(Y, X), 'b')
\end{lstlisting}

\begin{lstlisting}
[<matplotlib.lines.Line2D at 0x7fbbb0cdffd0>]
\end{lstlisting}

\begin{figure}
\centering
\includegraphics[width=0.66\linewidth]{output_45_1.png}
\caption[]{png}
\end{figure}

\begin{lstlisting}[language=Python]
# we have set gamma = 2*sqrt(2)
def hatpsi(_t):
    return \
        Heaviside(_t) * \
        2**(Rational(3, 4)) * \
        Matrix([
            [0, 0],
            [0, 1]
        ]) * \
        non_unitary_psi(_t)
        
def hatpsi_n(_t):
    return \
        np.heaviside(_t, 0) * \
        2**(3/4) * \
        np.array([
            [0, 0],
            [0, 1]
        ]) * \
        non_unitary_psi_n(_t)
\end{lstlisting}

\begin{lstlisting}[language=Python]
hatpsi(t)
\end{lstlisting}

\begin{equation}\label{eq:sympy:hatpsi}
\left[\begin{matrix}0\\2^{\frac{3}{4}} \left(\frac{\sqrt{2} e^{- \frac{\sqrt{2} t}{2} - \frac{\sqrt{2} i t}{2}}}{2} - \frac{\sqrt{2} e^{- \frac{\sqrt{2} t}{2} + \frac{\sqrt{2} i t}{2}}}{2}\right) \theta\left(t\right)\end{matrix}\right]
\end{equation}

\begin{lstlisting}[language=Python]
def hatpsisquarednorm(_t):
    return abs(hatpsi(_t)[0]**2) + abs(hatpsi(_t)[1]**2)

def hatpsisquarednorm_n(_t):
    return abs(hatpsi_n(_t)[0]**2) + abs(hatpsi_n(_t)[1]**2)
\end{lstlisting}

\begin{lstlisting}[language=Python]
hatpsisquarednorm(-1)
\end{lstlisting}

\[0\]

\begin{lstlisting}[language=Python]
plot(hatpsisquarednorm(t), (t, -2, 2*pi), line_color='m')
\end{lstlisting}

\begin{figure}
\centering
\includegraphics[width=0.66\linewidth]{output_49_0.png}
\caption[]{png}
\end{figure}

\begin{lstlisting}
<sympy.plotting.plot.Plot at 0x7fbbb0df4588>
\end{lstlisting}

\begin{lstlisting}[language=Python]
plot(prob_1_detect(t), hatpsisquarednorm(t), (t, -0.25, 8*pi))
\end{lstlisting}

\begin{figure}
\centering
\includegraphics[width=0.66\linewidth]{output_50_0.png}
\caption[]{png}
\end{figure}

\begin{lstlisting}
<sympy.plotting.plot.Plot at 0x7fbbb0c173c8>
\end{lstlisting}

\begin{lstlisting}[language=Python]
def prob_0_hatpsi(_t):
    return abs(hatpsi(_t)[0]**2) / (abs(hatpsi(_t)[0]**2) + abs(hatpsi(_t)[1]**2))
\end{lstlisting}

\begin{lstlisting}[language=Python]
def prob_1_hatpsi(_t):
    return abs(hatpsi(_t)[1]**2) / (abs(hatpsi(_t)[0]**2) + abs(hatpsi(_t)[1]**2))
\end{lstlisting}

\begin{lstlisting}[language=Python]
plot( abs(hatpsi(t)[1]**2), (t, -2, 2*pi), line_color='b')
\end{lstlisting}

\begin{figure}
\centering
\includegraphics[width=0.66\linewidth]{output_53_0.png}
\caption[]{png}
\end{figure}

\begin{lstlisting}
<sympy.plotting.plot.Plot at 0x7fbbb0bbf6d8>
\end{lstlisting}

\begin{lstlisting}[language=Python]
def fhatpsi1(_nu):
    return fourier_transform(hatpsi(t)[1], t, _nu)
\end{lstlisting}

\begin{lstlisting}[language=Python]
fhatpsi1(nu)
\end{lstlisting}

\begin{equation}\label{eq:fhatpsi1_nu}
- \frac{2^{\frac{3}{4}} i}{- 4 \pi^{2} \nu^{2} + 2 \sqrt{2} i \pi \nu + 1}
\end{equation}

\begin{lstlisting}[language=Python]
plot(abs(fhatpsi1(nu))**2, (nu, -1, 1), line_color='#bbbbbb')
\end{lstlisting}

\begin{figure}
\centering
\includegraphics[width=0.66\linewidth]{output_56_0.png}
\caption[]{png}
\end{figure}

\begin{lstlisting}
<sympy.plotting.plot.Plot at 0x7fbbb0a273c8>
\end{lstlisting}

The above Fourier transform is defined in frequency ($\nu$) not angular
frequency ($\omega$), therefore needs rescaling.

\begin{lstlisting}[language=Python]
def fhatpsiomega(_omega):
    return fhatpsi1(_omega/(2*pi)) / sqrt((2*pi))
\end{lstlisting}

\begin{lstlisting}[language=Python]
    fhatpsiomega(omega)
\end{lstlisting}

\begin{equation}\label{eq:fhatpsi1_omega}
    - \frac{\sqrt[4]{2} i}{\sqrt{\pi} \left(- \omega^{2} + \sqrt{2} i \omega + 1\right)}
\end{equation}

\begin{lstlisting}[language=Python]
    abs(fhatpsiomega(omega))**2
\end{lstlisting}

\begin{equation}\label{eq:fhatpsi1_omega_sqabs}
    - \frac{\sqrt{2}}{\pi \left(- \omega^{4} - 1\right)}
\end{equation}

\begin{lstlisting}[language=Python]
plot(abs(fhatpsiomega(omega))**2, (omega, -2*pi, 2*pi), line_color='magenta')
\end{lstlisting}

\begin{figure}
\centering
\includegraphics[width=0.66\linewidth]{output_59_0.png}
\caption[]{png}
\end{figure}

\begin{lstlisting}
<sympy.plotting.plot.Plot at 0x7fbbb0a82fd0>
\end{lstlisting}

\begin{lstlisting}[language=Python]
# graphical comparison with a normalized gaussian
sigma = 1.0
plot((1/(sqrt(2*pi)*sigma)) * exp(-omega**2/(2*(sigma)**2)), (omega, -2*pi, 2*pi), line_color='magenta')
\end{lstlisting}

\begin{figure}
\centering
\includegraphics[width=0.66\linewidth]{output_60_0.png}
\caption[]{png}
\end{figure}

\begin{lstlisting}
<sympy.plotting.plot.Plot at 0x7fbbb08c62b0>
\end{lstlisting}

\hypertarget{discrete-page-wootters-model}{%
\subsection{(Discrete) Page-Wootters
model}\label{discrete-page-wootters-model}}

\begin{lstlisting}[language=Python]
from scipy.linalg import dft, norm, expm
from scipy import stats
\end{lstlisting}

\begin{lstlisting}[language=Python]
T = np.diag(np.arange(0,32)) * np.pi / 16
\end{lstlisting}

\begin{lstlisting}[language=Python]
# The NumPy Fourier matrix is the conjugate of Mathematica's one,
# hence the trailing .conj() 
F = dft(32, scale='sqrtn').conj()
\end{lstlisting}

\begin{lstlisting}[language=Python]
F_dagger = F.conj().T
\end{lstlisting}

\begin{lstlisting}[language=Python]
Omega = F @ T @ F_dagger * 16 / np.pi
\end{lstlisting}

\begin{lstlisting}[language=Python]
oeigenvalues, oeigenvectors = np.linalg.eig(Omega)
\end{lstlisting}

\begin{lstlisting}[language=Python]
np.round(oeigenvalues)
\end{lstlisting}

\begin{lstlisting}
array([-0.+0.j, 31.+0.j,  1.+0.j, 30.+0.j,  2.+0.j, 29.-0.j,  3.+0.j,
       28.-0.j,  4.-0.j, 27.-0.j,  5.-0.j, 26.+0.j,  6.-0.j, 25.+0.j,
        7.-0.j,  8.+0.j, 24.+0.j,  9.-0.j, 23.+0.j, 10.+0.j, 22.+0.j,
       11.+0.j, 21.-0.j, 12.+0.j, 13.-0.j, 20.-0.j, 14.-0.j, 15.-0.j,
       19.+0.j, 16.-0.j, 17.+0.j, 18.-0.j])
\end{lstlisting}

\begin{lstlisting}[language=Python]
H = np.array([
    [0, 1],
    [1, 0]
])
\end{lstlisting}

\begin{lstlisting}[language=Python]
D = np.array([
    [0, 0],
    [0, np.sqrt(2)]
])
\end{lstlisting}

\begin{lstlisting}[language=Python]
K = H - 1j*D
\end{lstlisting}

\begin{lstlisting}[language=Python]
K
\end{lstlisting}

\begin{lstlisting}
array([[0.+0.j        , 1.+0.j        ],
       [1.+0.j        , 0.-1.41421356j]])
\end{lstlisting}

\begin{lstlisting}[language=Python]
J = np.kron(Omega, np.eye(2)) + np.kron(np.eye(32), K)
\end{lstlisting}

\begin{lstlisting}[language=Python]
eigenvalues, eigenvectors = np.linalg.eig(J)
\end{lstlisting}

\begin{lstlisting}[language=Python]
EnergyCorrectionMatrices = np.zeros((64, 64, 64), np.complex)
for n in range(64):
    EnergyCorrectionMatrices[n] = np.kron(
        expm(-1j*eigenvalues[n]*T),
        np.eye(2)
    )
# TODO: DRY
EnergyCorrectionMatricesT = np.zeros((64, 32, 32), np.complex)
for n in range(64):
    EnergyCorrectionMatricesT[n] = expm(-1j*eigenvalues[n]*T)
\end{lstlisting}

\begin{lstlisting}[language=Python]
def history_vector(eigenindex):
    # Needs matrix transposition ".T" (different convention as opposed to Mathematica)
    eigenvector = eigenvectors.T[eigenindex]
    return EnergyCorrectionMatrices[eigenindex] @ eigenvector

# "unflatten" the history_vector v into a a sequence of qubit component pairs
def reshape(v):
    return np.reshape(v, (-1,2))

# also make the first component real
def normalize_initial(v):
    vout = np.zeros(64, np.complex)
    # A phase factor to make it real
    vout = v * np.exp(-1j * np.angle(v[0]))
    # And a factor to normalize the initial state
    vout = vout / sqrt(
        np.abs(vout[0]**2) + np.abs(vout[1]**2)
    )
    return vout
\end{lstlisting}

\begin{lstlisting}[language=Python]
# Find the best linear combination to obtain |0> as initial state
def find_best():
    max_prob0 = 0
    max_prob0_i = 0
    max_prob0_j = 0
    for i in range(32):
        for j in range(32):
            qbi = reshape(history_vector(i))
            qbj = reshape(history_vector(j))
            qbit_hist = qbi + qbj
            prob0 = np.abs(qbit_hist[0][0]**2) / (
                np.abs(qbit_hist[0][0]**2) + np.abs(qbit_hist[0][1]**2)
            )
            if prob0 > max_prob0:
                max_prob0 = prob0
                max_prob0_i = i
                max_prob0_j = j
    print (max_prob0_i, max_prob0_j, max_prob0)
    return (max_prob0_i, max_prob0_j)
    
\end{lstlisting}

\begin{lstlisting}[language=Python]
# start with |0> as close as possible
i, j = find_best()
qbhistvec = normalize_initial(history_vector(i) + history_vector(j))
qbhist = reshape(qbhistvec) 
\end{lstlisting}

\begin{lstlisting}
1 21 1.0
\end{lstlisting}

\begin{lstlisting}[language=Python]
qbhist = qbhist.astype(complex)
\end{lstlisting}

Consitently with ``odinary QM'' findings, the component along
\textbar0\textgreater{} stays purely real, and the component along
\textbar1\textgreater{} stays purely imaginary.

\begin{lstlisting}[language=Python]
# Fill data for plotting
times = np.arange(0, 2*np.pi, np.pi/16)
norms = np.zeros(32)
probs0 = np.zeros(32)
probs1 = np.zeros(32)
# Components 0 are pure real, componets 1 are pure imag
real_parts0 = np.real(qbhist.T[0])
imag_parts1 = np.imag(qbhist.T[1])

for i in range(0, 32):
    norms[i] = (np.abs(qbhist[i][0]**2) + np.abs(qbhist[i][1]**2))
    probs0[i] = np.abs(qbhist[i][0]**2) / (
        np.abs(qbhist[i][0]**2) + np.abs(qbhist[i][1]**2) )
    probs1[i] = np.abs(qbhist[i][1]**2) / (
        np.abs(qbhist[i][0]**2) + np.abs(qbhist[i][1]**2) )
\end{lstlisting}

\begin{lstlisting}[language=Python]
plt.plot(times, norms/norms[0], 'g^')
\end{lstlisting}

\begin{lstlisting}
[<matplotlib.lines.Line2D at 0x7fbbab099710>]
\end{lstlisting}

\begin{figure}
\centering
\includegraphics[width=0.66\linewidth]{output_82_1.png}
\caption[]{png}
\end{figure}

\begin{lstlisting}[language=Python]
plt.plot(times, probs0, 'rs')
\end{lstlisting}

\begin{lstlisting}
[<matplotlib.lines.Line2D at 0x7fbbab078198>]
\end{lstlisting}

\begin{figure}
\centering
\includegraphics[width=0.66\linewidth]{output_83_1.png}
\caption[]{png}
\end{figure}

\begin{lstlisting}[language=Python]
plt.plot(times, probs1, 'bs')
\end{lstlisting}

\begin{lstlisting}
[<matplotlib.lines.Line2D at 0x7fbbaa9f7e48>]
\end{lstlisting}

\begin{figure}
\centering
\includegraphics[width=0.66\linewidth]{output_84_1.png}
\caption[]{png}
\end{figure}

\begin{lstlisting}[language=Python]
fig = plt.figure(figsize=(12,12))

#ax = fig.gca(projection='3d')
ax = fig.add_subplot(111, projection='3d')
ax.view_init(10,-45) # rotate 3d point of view

ax.scatter(
    real_parts0, imag_parts1, times
)

##ax.legend()

plt.xlabel('Re <0|\u03C8> (pure real)')
plt.ylabel('Im <1|\u03C8> (pure imag)')
ax.set_zlabel('t')
\end{lstlisting}

\begin{lstlisting}
Text(0.5, 0, 't')
\end{lstlisting}

\begin{figure}
\centering
\includegraphics[width=0.66\linewidth]{output_85_1.png}
\caption[]{png}
\end{figure}

\hypertarget{detection-event}{%
\subsection{Detection event}\label{detection-event}}

\begin{lstlisting}[language=Python]
sqr2D = np.array([
    [0, 0],
    [0, 2**(3/4)]
])
\end{lstlisting}

\begin{lstlisting}[language=Python]
qbhistvec = qbhistvec.astype(np.complex)
\end{lstlisting}

\begin{lstlisting}[language=Python]
sqr2D = sqr2D.astype(np.complex)
\end{lstlisting}

\begin{lstlisting}[language=Python]
#prob_detect_v = np.kron(np.eye(32), sqr2D) @ qbhistvec
#
# More direct route, without going through histories
i, j = find_best()
prob_detect_v = \
    (np.kron(EnergyCorrectionMatricesT[i], sqr2D) @ eigenvectors.T[i]) + \
    (np.kron(EnergyCorrectionMatricesT[j], sqr2D) @ eigenvectors.T[j])

# normalize
prob_detect_v = prob_detect_v / norm(prob_detect_v)

\end{lstlisting}

\begin{lstlisting}
1 21 1.0
\end{lstlisting}

\begin{lstlisting}[language=Python]
prob_detect = np.zeros(32)
for t_idx in range(32):
    prob_detect[t_idx] = \
        np.abs(prob_detect_v[2*t_idx])**2 + np.abs(prob_detect_v[2*t_idx+1])**2
\end{lstlisting}

\begin{lstlisting}[language=Python]
plt.plot(times, prob_detect * 16 / np.pi, 'bs')
\end{lstlisting}

\begin{lstlisting}
[<matplotlib.lines.Line2D at 0x7fbbaadcbb00>]
\end{lstlisting}

\begin{figure}
\centering
\includegraphics[width=0.66\linewidth]{output_92_1.png}
\caption[]{png}
\end{figure}

\begin{lstlisting}[language=Python]
detect_fft = \
    np.kron(F, np.eye(2)) @ prob_detect_v
detect_fft = detect_fft / norm(detect_fft)
\end{lstlisting}

\begin{lstlisting}[language=Python]
prob_detect_fft = np.zeros(32)
for o in range(32):
    prob_detect_fft[o] = \
        np.abs(detect_fft[2*o]**2) + \
        np.abs(detect_fft[2*o + 1]**2) 
\end{lstlisting}

\begin{lstlisting}[language=Python]
# Arrays are "rolled" because the second half 
# of the spectrum is identified with
# negative frequencies.
plt.plot(range(-16, 16), np.roll(prob_detect_fft, -16), 'y^')
\end{lstlisting}

\begin{lstlisting}
[<matplotlib.lines.Line2D at 0x7fbbaaf32e80>]
\end{lstlisting}

\begin{figure}
\centering
\includegraphics[width=0.66\linewidth]{output_95_1.png}
\caption[]{png}
\end{figure}

\begin{lstlisting}[language=Python]
S_t = stats.entropy(prob_detect)
\end{lstlisting}

\begin{lstlisting}[language=Python]
S_omega = stats.entropy(prob_detect_fft)
\end{lstlisting}

\begin{lstlisting}[language=Python]
S_t
\end{lstlisting}

\[2.6193337590390438\]

\begin{lstlisting}[language=Python]
S_omega
\end{lstlisting}

\[1.3471684169765332\]

\begin{lstlisting}[language=Python]
S_t * S_omega
\end{lstlisting}

\[3.528683713697821\]

\begin{lstlisting}[language=Python]
np.log(32)
\end{lstlisting}

\[3.4657359027997265\]

\begin{lstlisting}[language=Python]
(S_t * S_omega - np.log(32)) / np.log(32)
\end{lstlisting}

\[0.01816289892349938\]

Only 1.8\% more than the minumum per entropic uncertainty relation.

\hypertarget{use-scipy-routines-to-compute-sigmas}{%
\subsubsection{Use Scipy routines to compute
sigmas}\label{use-scipy-routines-to-compute-sigmas}}

\begin{lstlisting}[language=Python]

xk = range(-16,16)
pk = np.roll(prob_detect_fft, -16)
detect_fft_pdist = stats.rv_discrete(name='prob_detect_fft_minus16', values=(xk, pk))
\end{lstlisting}

\begin{lstlisting}[language=Python]
sigma_omega = detect_fft_pdist.std()
\end{lstlisting}

\begin{lstlisting}[language=Python]
xk = times
pk = prob_detect
detect_pdist = stats.rv_discrete(name='prob_detect', values=(xk, pk))
\end{lstlisting}

\begin{lstlisting}[language=Python]
sigma_t = detect_pdist.std()
\end{lstlisting}

\begin{lstlisting}[language=Python]
sigma_t * sigma_omega
\end{lstlisting}

\[0.7159703170687718\]

It's still quite a bit more than 0.5, i.e.~the minimum
uncertainty\ldots{}

But this is in fact consistent with the paper.
